# -*- coding: utf-8 -*-
"""apple-stock-prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1--olV5UKNDnTNPMTPRSu7obMBG7uKf0h
"""

# Description: This program uses an artificial recurrent neural network calls Long SHort Term Memory (LSTM)
# to predict the clsing stock price of a corporation (Aple Inc.) using the past 60 day stock price

# Import libraries
import math
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import yfinance as yf


plt.style.use('fivethirtyeight')

# Get the stock quote
# Use yfinanca to download data
data = yf.download("AAPL", start="2019-01-01", end="2024-11-27")

# Create a Pandas DataFrame
df = pd.DataFrame(data)

# Show the data
df

#Dropping Tickers
df.columns = df.columns.droplevel(level='Ticker')
df

# GEt the number of rows and columns in the data set
df.shape

#Visualize the closing price history
plt.figure(figsize=(16,8))
plt.title('Close Price History')
plt.plot(df['Close'])
plt.xlabel('Date', fontsize=18)
plt.ylabel('Close Price USD ($)', fontsize=18)
plt.show()

# Create a new datafram with only the 'Close' cloumn
data = df.filter(['Close'])
# Convert the dataframe to a numpy array
dataset = data.values
# Get the number of rows to train the model on
training_data_len = math.ceil(len(dataset) * .8)

training_data_len

# Scale the data
scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = scaler.fit_transform(dataset)

scaled_data

# Create the training dataset
# Crete the scaled training data set
train_data = scaled_data[0:training_data_len, :]
# Split the data into x_train and y_train data sets
x_train = [] #Independent variables
y_train = [] #target verial or dependent variables

for i in range(60, len(train_data)):
  x_train.append(train_data[i-60:i, 0])
  y_train.append(train_data[i, 0])
  if i<= 60:
    print(x_train)
    print(y_train)
    print()

# Convert the x_train and y_train to numpy arrays
x_train, y_train = np.array(x_train), np.array(y_train)

# Reshape the data (Reshaping because the LSTM model requires a 3D in form of number of samples, number of time steps, number of features)
# Samples: The number of independent time series or sequences you have in your data.
# Timestamps: The length of each sequence (number of time steps).
# Features: The number of features or variables you have at each time step.

x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
x_train.shape

# Build the LSTM model
# number of neurons = 5
#number os timestamps = 60
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape= (x_train.shape[1], 1)))

# Second LSTM layer
model.add(LSTM(50, return_sequences=False))

# Add 2 Dense layer
model.add(Dense(25))
model.add(Dense(1))

# Compile the model
# An optimizer is used to measure the loss function and the loss function is how well it did in training
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
#fit is another name for train
#batch size is the total number of training example present in a single batch
# epochs is the number of iteration when an entire dataset is past forward and backward through a nueral network
model.fit(x_train, y_train, batch_size=1, epochs=1)

# Create the testing data set
# Create a new array containing scaled values from index 1130 to 1487
test_data = scaled_data[training_data_len - 60: , :]
# Create the data sets x_test and y_test
x_test = []
y_test = dataset[training_data_len:]

for i in range(60, len(test_data)):
  x_test.append(test_data[i-60:i, 0])

# Convert the data to a numpy array
x_test = np.array(x_test)

#Reshape the data for 2D to 3D because LSTM requesires a 3D shaped data
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))

#Get the models predicted price values
predictions = model.predict(x_test)
predictions = scaler.inverse_transform(predictions)

#Get the root mean squared error (RMSE)
# RMSE is a good measure of how accurate the model predict teh response
# it is the standard deviation of the residuals
# the law value indicates a better fit

rmse = np.sqrt(np.mean(predictions - y_test)**2)
rmse

#Plot the data
train = data[:training_data_len]
valid = data[training_data_len:]
valid['Predictions'] = predictions


# Visualise the data
plt.figure(figsize=(16,8))
plt.title('Model')
plt.xlabel('Date', fontsize=18)
plt.ylabel('Close Price USD ($)', fontsize=18)
plt.plot(train['Close'])
plt.plot(valid[['Close', 'Predictions']])
plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')
plt.show()

#Show the valid and predicted prices
valid

# Get the quote
apple_qoute = yf.download("AAPL", start="2019-01-01", end="2024-11-27")

# Create a Pandas DataFrame
new_df = pd.DataFrame(apple_qoute)

#Dropping Tickers
new_df.columns = new_df.columns.droplevel(level='Ticker')

# Create a new dataframe
new_df = new_df.filter(['Close'])

# Get the last 60 day closing price values and convert the dataframe to an array
last_60_days = new_df[-60:].values

# Scale the data to be values between 0 and 1
last_60_days_scaled = scaler.transform(last_60_days)

#Create an empty list
X_test = []

# Append the past 60 days
X_test.append(last_60_days_scaled)

# Convert the X_test data set to a numpy array
X_test = np.array(X_test)

# Reshape the data
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

# Get the predicted scaled price
pred_price = model.predict(X_test)

#undo the scaling
pred_price = scaler.inverse_transform(pred_price)
print(pred_price)

# Get the quote
apple_qoute2 = yf.download("AAPL", start="2024-11-28", end="2024-11-28")
print(apple_qoute2['Close'])